{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68dc640",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8eef4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/insop/ML_crash_course/blob/main/2_linear_regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174c754",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "- recap from previous [notebook](https://github.com/insop/ML_crash_course/blob/main/1_ml_overview.ipynb)\n",
    "- linear regression framework\n",
    "- type: which predictor?\n",
    "- loss function: how good is the predictor?\n",
    "- optimization: how to compute the best predictor?\n",
    "- sample example\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4c5306",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "- ML as a way to predict target values or types by using training dataset.\n",
    "- Classification task predicts categories and \n",
    "- regression tasks predicts real numbers.\n",
    "\n",
    "- Supervised learning uses labeled dataset to train\n",
    "- Unsupervised learning tryies to cluster\n",
    "- a simple regression example\n",
    "\n",
    "- `numpy` and `pandas`.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa420a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Previously, we have reviewed ML as a way to predict target values or types by using training dataset. Classification task predicts categories and regression tasks predicts real numbers.\n",
    "\n",
    "For the type of training, supervised learning uses labeled dataset to train and unsupervised learning tryies to cluster or find implcit information from unlabeled dataset.\n",
    "We havre reviewed a simple regression task that predicts life expectancy based on GDP per capita\n",
    "\n",
    "For the tools, we have reviewed `numpy` and `pandas`.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c2bab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression\n",
    "\n",
    "- Whe we have these point pairs, can we predict $y$ for new $x$, such as 3?\n",
    "- $x$ :[1,2,4], $y$:[1,3,3]\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/linear_regression_example_1.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c8f80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression\n",
    "- if we can learn this linear line, then we can predict $y$ given new $x$\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/linear_regression_example_2.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b791364",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression\n",
    "\n",
    "- predicts values based on input dataset\n",
    "- $x$ (input) $\\rightarrow$ $f$ (predictor) $\\rightarrow$ $\\hat{y}$ (output)\n",
    "- How to design predictor?\n",
    "    - what type of predictor to use\n",
    "    - **loss function**: how to measure the goodness?\n",
    "    - **optimization**: how to compute the predictor? \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decfd058",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression\n",
    "\n",
    "- hypothesis predictor class: $w \\in \\mathbb{R}^2$\n",
    "- we want to find $f_w(x) = w_1 x + w_2$\n",
    "- **weight vector**: $W= [w_1, w_2]$\n",
    "- **feature vector**: $\\phi(x) = [x, 1]$\n",
    "- $f_w(x) = W \\cdot \\phi(x)$ = $w_1 x + w_2 1$\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/linear_regression_example_3.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca64ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss function: how good is a predictor?\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/linear_regression_example_4.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c64f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss function: how good is a predictor?\n",
    "\n",
    "- $Loss(x, y, W)$ = $(f_w(x) - y)^2$, **squared loss**, ($f_w(x) = w_1 x + w_2$)\n",
    "- Train_loss($W$) = $\\frac{1}{|D_{train}|}\\sum_{{(x,y)}\\in D_{train}} Loss(x, y, W)$\n",
    "- Example with $W = [0.7,1]$\n",
    "    - Loss($1,1,W$) = $(0.7\\times1 + 1 - 1)^2$\n",
    "    - Loss($2,3,W$) = $(0.7\\times2 + 3 - 3)^2$\n",
    "    - Loss($4,3,W$) = $(0.7\\times4 + 3 - 3)^2$\n",
    "    - Train_loss($W$) = $\\frac{1}{3}$(Loss($1,1,W$) + Loss($2,3,W$) + Loss($4,3,W$))\n",
    "    - See the example code below\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c23f7364",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total loss for [0.7, 1]: 0.4966666666666666\n",
      "\n",
      "Total loss for [0.3, 2]: 0.6299999999999998\n",
      "\n",
      "Total loss for [-1, -1]: 36.333333333333336\n"
     ]
    }
   ],
   "source": [
    "# Loss calculation example\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "training_data = {\n",
    "    'x':[1,2,4], \n",
    "    'y':[1,3,3]}\n",
    "\n",
    "# [0.7, 1]: gree\n",
    "# [0.3, 2]: red\n",
    "# [0.57, 1]: other\n",
    "Ws = [[0.7, 1], [0.3, 2], [-1,-1]]\n",
    "\n",
    "def phi(x):\n",
    "    \"\"\"Get feature of x\"\"\"\n",
    "    return np.array([x, 1])\n",
    "\n",
    "def dot(X, Y):\n",
    "    \"\"\" Do the dot product \"\"\"\n",
    "    return sum([x*y for x, y in zip(X, Y)])\n",
    "\n",
    "for w in Ws:\n",
    "    \"\"\"Evaluate w's loss\"\"\"\n",
    "    print()\n",
    "    losses = []\n",
    "    for i,(x,y) in enumerate(zip(training_data['x'], training_data['y'])):\n",
    "        phi_x = phi(x)\n",
    "\n",
    "        loss = (dot(w, phi_x) - y)**2\n",
    "        losses.append(loss)\n",
    "\n",
    "#         print(\"loss_{}: {}\".format(i, loss))\n",
    "    print(\"Total loss for {}: {}\".format(w, sum(losses)/len(losses)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627207c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing loss function\n",
    "\n",
    "- Train_loss($W$) = $\\frac{1}{|D_{train}|}\\sum_{{(x,y)}\\in D_{train}} Loss(x, y, W)$\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/loss_visualization.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Figure is from Ref[2]\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a895bdf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization: how to find the best $W$?\n",
    "- **Goal**: find *minimum* Train_loss($W$)\n",
    "    - min$_w$ Train_loss($W$)\n",
    "- **gradient**: the gradient $\\nabla_w$ Train_loss($W$) is the direction that *increases* the training loss the most\n",
    "- **Gradient descent** algorithm\n",
    "    - initialize $w$ = [0, ... 0]\n",
    "    - set $\\eta$, step size (learning rate)\n",
    "    - For t = 1, ..., T: # called epochs\n",
    "        - $w \\leftarrow w - \\eta$ $\\nabla$ Train_loss($W$) \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d8029",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing the gradient\n",
    "\n",
    "- Train_loss($W$) = $\\frac{1}{|D_{train}|}\\sum_{{(x,y)}\\in D_{train}} (W \\cdot \\phi(x) - y)^2$\n",
    "\n",
    "- Gradient\n",
    "    - $\\nabla_w$ Train_loss($W$) = $\\frac{1}{|D_{train}|}\\sum_{{(x,y)}\\in D_{train}} 2(W \\cdot \\phi(x) - y)\\phi(x)$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6b3574",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We are doing gradient with respect $W$, so other terms can be considered as constant, then we can apply chain rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff8c4fe",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final weight: [0.57142857 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent example\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "training_data = {\n",
    "    'x':[1,2,4], \n",
    "    'y':[1,3,3]}\n",
    "\n",
    "def phi(x):\n",
    "    \"\"\"Get feature of x\"\"\"\n",
    "    return np.array([x, 1])\n",
    "\n",
    "def dot(X, Y):\n",
    "    \"\"\" Do the dot product \"\"\"\n",
    "    return sum([x*y for x, y in zip(X, Y)])\n",
    "\n",
    "debug=False\n",
    "eta = 0.1\n",
    "w = np.array([0, 0])\n",
    "\n",
    "for t in range(500):\n",
    "    gradients = []\n",
    "    for i, (x,y) in enumerate(zip(training_data['x'], training_data['y'])):\n",
    "        phi_x = phi(x)\n",
    "        gradient = (2*(dot(w, phi_x)-y)*phi_x)\n",
    "        gradients.append(gradient)\n",
    "    w = w - eta* sum(gradients)/len(gradients)\n",
    "    if debug:\n",
    "        print(sum(gradients)/len(gradients), w)\n",
    "\n",
    "print()\n",
    "print(\"Final weight: {}\".format(w))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412b994",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (?) Features and vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2cdfd2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "- review linear regression\n",
    "- how to form linear regression\n",
    "- loss function, how to measure goodness of the hypothesis\n",
    "- optimization, how to find the best parameters (weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92674d66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Credits\n",
    "\n",
    "This notebook uses the contents from the followring materials:\n",
    "\n",
    "1. [cs221 ML linear regression](https://stanford-cs221.github.io/autumn2021-extra/modules/machine-learning/linear-regression.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cbc932",
   "metadata": {},
   "source": [
    "## Further readings\n",
    "\n",
    "1. Chapter 1 from Book [Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
    "1. [cs221 ML linear regression](https://stanford-cs221.github.io/autumn2021-extra/modules/machine-learning/linear-regression.pdf)\n",
    "1. [Jovian's Linear Regression with Scikit Learn](https://jovian.ai/learn/machine-learning-with-python-zero-to-gbms/lesson/linear-regression-with-scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a16bc9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
